{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7176604,"sourceType":"datasetVersion","datasetId":4147303}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORT PACKAGES AND DATA","metadata":{}},{"cell_type":"code","source":"# Fix randomness and hide warnings\nseed = 42\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nimport numpy as np\nnp.random.seed(seed)\n\nimport logging\n\nimport random\nrandom.seed(seed)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\nfrom tensorflow.keras import initializers\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=np.load('/kaggle/input/data-assignment2/training_data.npy', allow_pickle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories=np.load('/kaggle/input/data-assignment2/categories.npy', allow_pickle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPLORE DATA","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n# Get unique class labels and their counts\nunique_classes, class_counts = np.unique(categories, return_counts=True)\nprint(class_counts)\n\n# Compute class weights\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(categories), y=categories)\n\n# Create a dictionary mapping class indices to their respective weights\nclass_weights_dict = dict(zip(range(len(class_weights)), class_weights))\n\nprint(\"Class weights:\", class_weights_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_periods=np.load('/kaggle/input/data-assignment2/valid_periods.npy', allow_pickle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recovered_series = []\n\nfor i in range(len(data)):\n    start, end = valid_periods[i]\n    series = data[i][start:end]\n    recovered_series.append(series)\n\ncategorized_series = {category: [] for category in ['A', 'B', 'C', 'D', 'E', 'F']}\n\nfor i, category_code in enumerate(categories):\n    category = category_code.item()  # Extract the string value\n    categorized_series[category].append(recovered_series[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quick_counter = 0\nn_samples_cat = []\nfor category in ['A', 'B', 'C', 'D', 'E', 'F']:\n    num_cat = len(categorized_series[category])\n    quick_counter += num_cat\n    print(f'There are {num_cat} time series of category {category}')\n    n_samples_cat.append(num_cat)\n    \ncategories = ['A', 'B', 'C', 'D', 'E', 'F']\nnum_time_series = [len(categorized_series[category]) for category in categories]\n\nplt.figure(figsize=(8, 6))\nplt.bar(categories, num_time_series, color='skyblue')\nplt.xlabel('Categories')\nplt.ylabel('Number of Time Series')\nplt.title('Number of Time Series in Each Category')\nplt.grid(axis='y')\nplt.show()\n    \nprint(quick_counter) # Check if everything's fine\nprint(n_samples_cat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access the first 10 elements of each category\nfirst_10_elements = {category: categorized_series[category][:10] for category in categorized_series}\n\n# Print the first 10 elements of each category along with their lengths\nfor category, elements in first_10_elements.items():\n    print(f\"Category {category}:\")\n    for idx, element in enumerate(elements, start=1):\n        print(f\"Element {idx}: Length - {len(element)}\")\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the first 10 elements of each category on different figures\nfor category, elements in categorized_series.items():\n    plt.figure(figsize=(8, 6))\n    plt.title(f\"Category {category}\")\n    \n    for idx, element in enumerate(elements[:10], start=1):\n        plt.subplot(5, 2, idx)  # Create subplots for each element\n        plt.plot(element)\n        plt.title(f\"Element {idx}\")\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nall_train_data = []\nall_test_data = []\nall_val_data = []\n\nfor category_data in [categorized_series['A'], categorized_series['B'], categorized_series['C'], categorized_series['D'], categorized_series['E'], categorized_series['F']]:  # Add other categories as needed\n    # Perform train-test-validation split for each category\n    train_data, val_data = train_test_split(category_data, test_size=0.25, shuffle=True , random_state=2)\n    #test_data, val_data = train_test_split(test_val_data, test_size=0.2, shuffle=True)  # Split the remaining for test and val\n    \n    # Append data from each category to the respective lists\n    all_train_data.extend(train_data)\n    # all_test_data.extend(test_data)\n    all_val_data.extend(val_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_length = 200\n\nnp.random.shuffle(all_train_data)\nnp.random.shuffle(all_val_data)\n#np.random.shuffle(all_test_data)\n\ndef create_sequences(data):\n    input_sequences = []\n    output_sequences = []\n    for series in data:\n        for i in range(len(series) - sequence_length - 9):  # Considering 9 samples as the prediction horizon\n            input_sequences.append(series[i:i + sequence_length])\n            output_sequences.append(series[i + sequence_length:i + sequence_length + 9])\n    return np.array(input_sequences), np.array(output_sequences)\n\n# Create input-output sequences for training, validation, and testing data\ntrain_input, train_output = create_sequences(all_train_data)\nval_input, val_output = create_sequences(all_val_data)\n#test_input, test_output = create_sequences(all_test_data)\n\n# Reshape data for RNN input (assuming 3D input shape [samples, time steps, features])\ntrain_input = train_input.reshape(train_input.shape[0], sequence_length)\nval_input = val_input.reshape(val_input.shape[0], sequence_length)\n#test_input = test_input.reshape(test_input.shape[0], sequence_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\n\nfrom keras import regularizers\nfrom tensorflow.keras.layers import MultiHeadAttention, Dropout, LayerNormalization\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, GlobalAveragePooling1D, Concatenate, MaxPooling1D\n\nimport tensorflow as tf\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RESNET + BIDIRECTIONAL LSTM","metadata":{}},{"cell_type":"code","source":"def build_model(input_shape=(200, 1), output_length=9):\n    n_feature_maps = 16\n\n    input_layer = keras.layers.Input(input_shape)\n\n    # BLOCK 1\n\n    conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n    # expand channels for the sum\n    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n    \n    # BLOCK 2\n\n    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n    # expand channels for the sum\n    shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n\n    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n \n    # BLOCK 3\n\n    conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n    conv_x = keras.layers.BatchNormalization()(conv_x)\n    conv_x = keras.layers.Activation('relu')(conv_x)\n\n    conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n    conv_y = keras.layers.BatchNormalization()(conv_y)\n    conv_y = keras.layers.Activation('relu')(conv_y)\n\n    conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n    conv_z = keras.layers.BatchNormalization()(conv_z)\n\n    # no need to expand channels because they are equal\n    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n\n    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n       \n    # Use GlobalAveragePooling1D to aggregate features before LSTM\n    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n\n    # Reshape for LSTM layer\n    lstm_input = keras.layers.Reshape((-1, n_feature_maps * 2))(gap_layer)\n\n    # Bidirectional LSTM layers for forecasting\n    lstm_layer1 = keras.layers.Bidirectional(keras.layers.LSTM(units=128, return_sequences=True))(lstm_input)\n    lstm_layer2 = keras.layers.Bidirectional(keras.layers.LSTM(units=64))(lstm_layer1)\n        \n    # Output layer for forecasting the next 9 samples\n    output_layer = keras.layers.Dense(output_length)(lstm_layer2)\n\n    # Create the model\n    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n\n    return model\n\n# Build the model\nmodel = build_model(input_shape=(200, 1), output_length=9)\n\n# Compile the model\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n\n# Display model summary\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_input, train_output, epochs=15, batch_size=128, validation_data=(val_input, val_output), callbacks=[tfk.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True), tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}